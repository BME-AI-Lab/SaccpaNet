{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_val: 0.4832, auc_val: 0.8820\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.45      0.15      0.22        96\n",
      "           1       0.44      0.50      0.47        96\n",
      "           2       0.56      0.88      0.69        96\n",
      "           3       0.64      0.60      0.62        96\n",
      "           4       0.77      0.93      0.84        96\n",
      "           5       0.25      0.03      0.06        96\n",
      "           6       0.39      0.67      0.49        96\n",
      "\n",
      "    accuracy                           0.54       672\n",
      "   macro avg       0.50      0.54      0.48       672\n",
      "weighted avg       0.50      0.54      0.48       672\n",
      "\n",
      "[[14 27  8  6  0  4 37]\n",
      " [ 0 48 44  3  0  0  1]\n",
      " [ 1 11 84  0  0  0  0]\n",
      " [ 0  0  1 58 26  4  7]\n",
      " [ 0  0  0  7 89  0  0]\n",
      " [ 9 11  6 11  1  3 55]\n",
      " [ 7 12  6  6  0  1 64]]\n",
      "f1_test: 0.4115, auc_test: 0.8559\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       120\n",
      "           1       0.40      0.88      0.55       120\n",
      "           2       0.75      0.69      0.72       120\n",
      "           3       0.47      0.42      0.45       120\n",
      "           4       1.00      0.56      0.72       120\n",
      "           5       0.16      0.06      0.09       120\n",
      "           6       0.27      0.55      0.36       120\n",
      "\n",
      "    accuracy                           0.45       840\n",
      "   macro avg       0.44      0.45      0.41       840\n",
      "weighted avg       0.44      0.45      0.41       840\n",
      "\n",
      "array([[  0,  42,   3,   6,   0,   4,  65],\n",
      "       [  0, 106,  10,   0,   0,   0,   4],\n",
      "       [  0,  37,  83,   0,   0,   0,   0],\n",
      "       [  0,  11,   0,  51,   0,  23,  35],\n",
      "       [  0,   1,   6,  45,  67,   0,   1],\n",
      "       [  0,  33,   4,   2,   0,   7,  74],\n",
      "       [  0,  35,   5,   4,   0,  10,  66]], dtype=int64)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\tools\\miniconda3\\envs\\pytorch-mamba\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\tools\\miniconda3\\envs\\pytorch-mamba\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\tools\\miniconda3\\envs\\pytorch-mamba\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import sklearn.preprocessing\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "train_df = pd.read_pickle(\"train.pkl\")\n",
    "test_df = pd.read_pickle(\"test.pkl\")\n",
    "validation_df = pd.read_pickle(\"validation.pkl\")\n",
    "onehot = sklearn.preprocessing.OneHotEncoder()\n",
    "\n",
    "\n",
    "X_train = train_df[\"feature\"].values\n",
    "X_train = np.stack(X_train)\n",
    "Y_train = train_df[\"meta\"].values\n",
    "X_val = validation_df[\"feature\"].values\n",
    "X_val = np.stack(X_val)\n",
    "Y_val = validation_df[\"meta\"].values\n",
    "X_test = test_df[\"feature\"].values\n",
    "X_test = np.stack(X_test)\n",
    "Y_test = test_df[\"meta\"].values\n",
    "\n",
    "\n",
    "clf = SVC(probability=True)\n",
    "clf.fit(X_train, Y_train)\n",
    "result_val = clf.predict_proba(X_val)\n",
    "Y_val_onehot = onehot.fit_transform(Y_val.reshape(-1, 1)).toarray()\n",
    "f1_val = sklearn.metrics.f1_score(Y_val, result_val.argmax(axis=1), average=\"macro\")\n",
    "auc_val = sklearn.metrics.roc_auc_score(\n",
    "    Y_val_onehot, result_val, average=\"macro\", multi_class=\"ovr\"\n",
    ")\n",
    "print(f\"f1_val: {f1_val:.4f}, auc_val: {auc_val:.4f}\")\n",
    "classification_report = sklearn.metrics.classification_report(\n",
    "    Y_val, result_val.argmax(axis=1)\n",
    ")\n",
    "print(classification_report)\n",
    "confusion_matrix = sklearn.metrics.confusion_matrix(Y_val, result_val.argmax(axis=1))\n",
    "print(confusion_matrix)\n",
    "\n",
    "result_test = clf.predict_proba(X_test)\n",
    "Y_test_onehot = onehot.fit_transform(Y_test.reshape(-1, 1)).toarray()\n",
    "f1_test = sklearn.metrics.f1_score(Y_test, result_test.argmax(axis=1), average=\"macro\")\n",
    "auc_test = sklearn.metrics.roc_auc_score(\n",
    "    Y_test_onehot, result_test, average=\"macro\", multi_class=\"ovr\"\n",
    ")\n",
    "print(f\"f1_test: {f1_test:.4f}, auc_test: {auc_test:.4f}\")\n",
    "\n",
    "classification_report = sklearn.metrics.classification_report(\n",
    "    Y_test, result_test.argmax(axis=1)\n",
    ")\n",
    "print(classification_report)\n",
    "confusion_matrix = sklearn.metrics.confusion_matrix(Y_test, result_test.argmax(axis=1))\n",
    "print(repr(confusion_matrix))\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    240\n",
      "2    240\n",
      "3    240\n",
      "0    120\n",
      "Name: meta, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "test_df = pd.read_pickle(\"test.pkl\")\n",
    "\n",
    "# collapse data\n",
    "mapping_dict = {\n",
    "    0: 0,\n",
    "    1: 1,\n",
    "    2: 1,\n",
    "    3: 2,\n",
    "    4: 2,\n",
    "    5: 3,\n",
    "    6: 3,\n",
    "}\n",
    "# train_df[\"meta\"] = train_df[\"meta\"].map(mapping_dict)\n",
    "test_df[\"meta\"] = test_df[\"meta\"].map(mapping_dict)\n",
    "X_test = test_df[\"feature\"].values\n",
    "X_test = np.stack(X_test)\n",
    "Y_test = test_df[\"meta\"].values\n",
    "\n",
    "print(test_df[\"meta\"].value_counts())\n",
    "# result_test = clf.predict_proba(X_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0\n",
      "1 1\n",
      "2 1\n",
      "3 2\n",
      "4 2\n",
      "5 3\n",
      "6 3\n",
      "[[1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " ...\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]]\n",
      "[120. 240. 240. 240.]\n",
      "4 classes f1_test: 0.5544, auc_test: 0.8747\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       120\n",
      "           1       0.81      0.97      0.89       240\n",
      "           2       0.97      0.54      0.70       240\n",
      "           3       0.50      0.88      0.64       240\n",
      "\n",
      "    accuracy                           0.68       840\n",
      "   macro avg       0.57      0.60      0.55       840\n",
      "weighted avg       0.65      0.68      0.63       840\n",
      "\n",
      "array([[  0,  20,   0, 100],\n",
      "       [  0, 233,   0,   7],\n",
      "       [  0,   7, 130, 103],\n",
      "       [  0,  26,   4, 210]], dtype=int64)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\tools\\miniconda3\\envs\\pytorch-mamba\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\tools\\miniconda3\\envs\\pytorch-mamba\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\tools\\miniconda3\\envs\\pytorch-mamba\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# merge result_test according to the mapping_dict\n",
    "\n",
    "\n",
    "new_result = np.zeros((result_test.shape[0], 4))\n",
    "for i in range(7):\n",
    "    print(i, mapping_dict[i])\n",
    "    new_result[:,mapping_dict[i]] += result_test[:,i]\n",
    "#print(new_result)\n",
    "#print(new_result.sum(axis=1))\n",
    "    \n",
    "Y_test_onehot = onehot.fit_transform(Y_test.reshape(-1, 1)).toarray()\n",
    "print(Y_test_onehot)\n",
    "print(Y_test_onehot.sum(axis=0))\n",
    "f1_test = sklearn.metrics.f1_score(Y_test, new_result.argmax(axis=1), average=\"macro\")\n",
    "auc_test = sklearn.metrics.roc_auc_score(\n",
    "    Y_test_onehot, new_result, average=\"macro\", multi_class=\"ovr\"\n",
    ")\n",
    "print(f\"4 classes f1_test: {f1_test:.4f}, auc_test: {auc_test:.4f}\")\n",
    "classification_report = sklearn.metrics.classification_report(\n",
    "    Y_test, new_result.argmax(axis=1)\n",
    ")\n",
    "print(classification_report)\n",
    "confusion_matrix = sklearn.metrics.confusion_matrix(Y_test, new_result.argmax(axis=1))\n",
    "print(repr(confusion_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([[  0.,  42.,   6.,  72.],\n",
      "       [  0., 236.,   0.,   4.],\n",
      "       [  0.,  12., 162.,  66.],\n",
      "       [  0.,  69.,   6., 165.]])\n"
     ]
    }
   ],
   "source": [
    "array = np.array([[  0,  38,   4,   6,   0,  47,  25],\n",
    "       [  0, 106,  10,   0,   0,   4,   0],\n",
    "       [  0,  37,  83,   0,   0,   0,   0],\n",
    "       [  0,   5,   0,  50,   0,  60,   5],\n",
    "       [  0,   1,   6,  44,  68,   0,   1],\n",
    "       [  0,  28,   4,   2,   0,  61,  25],\n",
    "       [  0,  32,   5,   4,   0,  57,  22]], dtype=np.int64)\n",
    "new_result = np.zeros((array.shape[0], 4))\n",
    "for i in range(7):\n",
    "    #print(i, mapping_dict[i])\n",
    "    new_result[:,mapping_dict[i]] += array[:,i]\n",
    "array2 = new_result\n",
    "new_result = np.zeros((4, 4))\n",
    "for i in range(7):\n",
    "    #print(i, mapping_dict[i])\n",
    "    new_result[mapping_dict[i],:] += array2[i,:]\n",
    "print(repr(new_result))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_val: 0.6302, auc_val: 0.9189\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        96\n",
      "           1       0.82      0.98      0.90       192\n",
      "           2       0.98      0.84      0.91       192\n",
      "           3       0.61      0.88      0.72       192\n",
      "\n",
      "    accuracy                           0.77       672\n",
      "   macro avg       0.60      0.68      0.63       672\n",
      "weighted avg       0.69      0.77      0.72       672\n",
      "\n",
      "[[  0  22   0  74]\n",
      " [  0 188   0   4]\n",
      " [  0   0 162  30]\n",
      " [  1  18   4 169]]\n",
      "f1_test: 0.5643, auc_test: 0.8846\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       120\n",
      "           1       0.85      0.96      0.90       240\n",
      "           2       0.97      0.55      0.70       240\n",
      "           3       0.51      0.91      0.65       240\n",
      "\n",
      "    accuracy                           0.69       840\n",
      "   macro avg       0.58      0.61      0.56       840\n",
      "weighted avg       0.67      0.69      0.64       840\n",
      "\n",
      "array([[  0,  16,   0, 104],\n",
      "       [  0, 231,   0,   9],\n",
      "       [  0,   7, 132, 101],\n",
      "       [  0,  17,   4, 219]], dtype=int64)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\tools\\miniconda3\\envs\\pytorch-mamba\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\tools\\miniconda3\\envs\\pytorch-mamba\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\tools\\miniconda3\\envs\\pytorch-mamba\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import sklearn.preprocessing\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "train_df = pd.read_pickle(\"train.pkl\")\n",
    "test_df = pd.read_pickle(\"test.pkl\")\n",
    "validation_df = pd.read_pickle(\"validation.pkl\")\n",
    "onehot = sklearn.preprocessing.OneHotEncoder()\n",
    "\n",
    "# collapse data\n",
    "mapping_dict = {\n",
    "    0: 0,\n",
    "    1: 1,\n",
    "    2: 1,\n",
    "    3: 2,\n",
    "    4: 2,\n",
    "    5: 3,\n",
    "    6: 3,\n",
    "}\n",
    "train_df[\"meta\"] = train_df[\"meta\"].map(mapping_dict)\n",
    "test_df[\"meta\"] = test_df[\"meta\"].map(mapping_dict)\n",
    "validation_df[\"meta\"] = validation_df[\"meta\"].map(mapping_dict)\n",
    "\n",
    "X_train = train_df[\"feature\"].values\n",
    "X_train = np.stack(X_train)\n",
    "Y_train = train_df[\"meta\"].values\n",
    "X_val = validation_df[\"feature\"].values\n",
    "X_val = np.stack(X_val)\n",
    "Y_val = validation_df[\"meta\"].values\n",
    "X_test = test_df[\"feature\"].values\n",
    "X_test = np.stack(X_test)\n",
    "Y_test = test_df[\"meta\"].values\n",
    "\n",
    "clf = SVC(probability=True)\n",
    "clf.fit(X_train, Y_train)\n",
    "result_val = clf.predict_proba(X_val)\n",
    "Y_val_onehot = onehot.fit_transform(Y_val.reshape(-1, 1)).toarray()\n",
    "f1_val = sklearn.metrics.f1_score(Y_val, result_val.argmax(axis=1), average=\"macro\")\n",
    "auc_val = sklearn.metrics.roc_auc_score(\n",
    "    Y_val_onehot, result_val, average=\"macro\", multi_class=\"ovr\"\n",
    ")\n",
    "print(f\"f1_val: {f1_val:.4f}, auc_val: {auc_val:.4f}\")\n",
    "classification_report = sklearn.metrics.classification_report(\n",
    "    Y_val, result_val.argmax(axis=1)\n",
    ")\n",
    "print(classification_report)\n",
    "confusion_matrix = sklearn.metrics.confusion_matrix(Y_val, result_val.argmax(axis=1))\n",
    "print(confusion_matrix)\n",
    "\n",
    "result_test = clf.predict_proba(X_test)\n",
    "Y_test_onehot = onehot.fit_transform(Y_test.reshape(-1, 1)).toarray()\n",
    "f1_test = sklearn.metrics.f1_score(Y_test, result_test.argmax(axis=1), average=\"macro\")\n",
    "auc_test = sklearn.metrics.roc_auc_score(\n",
    "    Y_test_onehot, result_test, average=\"macro\", multi_class=\"ovr\"\n",
    ")\n",
    "print(f\"f1_test: {f1_test:.4f}, auc_test: {auc_test:.4f}\")\n",
    "\n",
    "classification_report = sklearn.metrics.classification_report(\n",
    "    Y_test, result_test.argmax(axis=1)\n",
    ")\n",
    "print(classification_report)\n",
    "confusion_matrix = sklearn.metrics.confusion_matrix(Y_test, result_test.argmax(axis=1))\n",
    "print(repr(confusion_matrix))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_test: 0.5435\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.0000    0.0000    0.0000       120\n",
      "           1     0.6574    0.9833    0.7880       240\n",
      "           2     0.9310    0.6750    0.7826       240\n",
      "           3     0.5375    0.6875    0.6033       240\n",
      "\n",
      "    accuracy                         0.6702       840\n",
      "   macro avg     0.5315    0.5865    0.5435       840\n",
      "weighted avg     0.6074    0.6702    0.6211       840\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\tools\\miniconda3\\envs\\pytorch-mamba\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\tools\\miniconda3\\envs\\pytorch-mamba\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\tools\\miniconda3\\envs\\pytorch-mamba\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "result_merged=  result_test.argmax(axis=1)\n",
    "result_merged = np.array([mapping_dict[i] for i in result_merged])\n",
    "f1_test = sklearn.metrics.f1_score(Y_test, result_merged, average=\"macro\")\n",
    "print(f\"f1_test: {f1_test:.4f}\")\n",
    "classification_report = sklearn.metrics.classification_report(\n",
    "    Y_test, result_merged,digits = 4\n",
    ")\n",
    "print(classification_report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-mamba",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
